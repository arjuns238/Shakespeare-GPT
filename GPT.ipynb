{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1a4fb03b-bec5-40c1-8b56-36c7241ddda0",
      "metadata": {
        "id": "1a4fb03b-bec5-40c1-8b56-36c7241ddda0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import Iterable, List\n",
        "import torchtext\n",
        "from tqdm import tqdm\n",
        "# # We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
        "# # Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
        "# multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "# multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}"
      ],
      "metadata": {
        "id": "2PB_QfL5G1qv"
      },
      "id": "2PB_QfL5G1qv",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "61cORS_g1wF7"
      },
      "id": "61cORS_g1wF7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "block_size = 256\n",
        "learning_rate = 1e-2\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "n_embd = 384\n",
        "dropout = 0.2\n",
        "no_of_heads = 6\n",
        "n_layer = 6\n",
        "device\n",
        "SRC_LANGUAGE = 'Fr'\n",
        "TGT_LANGUAGE = 'En'"
      ],
      "metadata": {
        "id": "TbgbFLV0pqLD"
      },
      "id": "TbgbFLV0pqLD",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Installing dependencies\n",
        "# !pip install -U torchdata\n",
        "# !pip install -U spacy\n",
        "# !pip install 'portalocker>=2.0.0'\n",
        "# # !python -m spacy download en_core_web_sm\n",
        "# !python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "id": "tgCic8ilHKyF"
      },
      "id": "tgCic8ilHKyF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "4684865e-141b-4753-91a7-b3a264165d2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4684865e-141b-4753-91a7-b3a264165d2d",
        "outputId": "b9e41716-6690-49a7-b4ae-a88a3974f4d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/input.txt\", \"r\") as f:\n",
        "     text = f.read()\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUhdVaS767gG",
        "outputId": "8490da49-ab0d-4d60-8d54-d49796462df2"
      },
      "id": "QUhdVaS767gG",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f6f55d5-421f-4bfd-8dfd-9c34053bbfb3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f6f55d5-421f-4bfd-8dfd-9c34053bbfb3",
        "outputId": "9b7035f6-ab69-4b55-aadc-8b9688e30056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "# Mapping between integers and characters\n",
        "stoi = {}\n",
        "itos = {}\n",
        "for i, ch in enumerate(chars):\n",
        "    stoi[ch] = i\n",
        "    itos[i] = ch\n",
        "\n",
        "# Take a string and output a list of integers\n",
        "def encode(s):\n",
        "    out = []\n",
        "    for ch in s:\n",
        "        out.append(stoi[ch])\n",
        "    return out\n",
        "\n",
        "# Take a list of integers and output a string\n",
        "def decode(ints):\n",
        "    out = \"\"\n",
        "    for i in ints:\n",
        "        out += itos[i]\n",
        "    return out\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "636de7f4-81b3-4a84-916a-c04d72baa8a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "636de7f4-81b3-4a84-916a-c04d72baa8a1",
        "outputId": "3818a7cc-65b1-4cdd-a1c8-4461e37058e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ],
      "source": [
        "# Encoding the entire dataset and saving it in a tensor\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjqjdBRu3Gkd",
        "outputId": "c878e6d6-743a-46f9-f84d-4b04a3cb2f67"
      },
      "id": "pjqjdBRu3Gkd",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([288721])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dcad20f-e57e-4f82-99da-e9d9224bb38e",
      "metadata": {
        "id": "5dcad20f-e57e-4f82-99da-e9d9224bb38e"
      },
      "outputs": [],
      "source": [
        "# Splitting into train and validation\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "732f2360-c7c4-4c47-89a2-271a9cd2cbfe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "732f2360-c7c4-4c47-89a2-271a9cd2cbfe",
        "outputId": "3458709c-15e6-4f78-a446-2cd5854c2a76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
              "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
              "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
              "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
              "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
              "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
              "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
              "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
              "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
              "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
              "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
              "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
              "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
              "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
              "        50, 50, 10,  0, 35])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Creating a block size (Maximum context)\n",
        "data[:block_size + 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9515571e-a758-4fde-b7ef-954ed3f48623",
      "metadata": {
        "id": "9515571e-a758-4fde-b7ef-954ed3f48623"
      },
      "outputs": [],
      "source": [
        "x = data[:block_size]\n",
        "y = data[1:block_size + 1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t + 1]\n",
        "    target = y[t]\n",
        "    print(f\"When context is {context}, target is {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bffb75d-c3a6-44a6-8c64-16f5f258a185",
      "metadata": {
        "id": "1bffb75d-c3a6-44a6-8c64-16f5f258a185"
      },
      "outputs": [],
      "source": [
        "gen = torch.manual_seed(1337)\n",
        "# Creating a batch size (Number of independent sequences processed in parallel)\n",
        "# batch_size = 4\n",
        "# block_size = 8\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    # ix is 4 (batch_size) numbers that are randomly generated between len(data) and block_size\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size, 1), generator = gen)\n",
        "    x = torch.stack([data[i: i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x,y\n",
        "xb, yb = get_batch(train_data)\n",
        "print(\"inputs:\")\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print(\"targets:\")\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "for b in range(batch_size):\n",
        "    for t in range(block_size):\n",
        "        context = xb[b, :t + 1]\n",
        "        target = yb[b, t]\n",
        "        print(f\"When context is {context.tolist()}, target is {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb[0])\n",
        "print(yb[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j82ZbrpiGTQ",
        "outputId": "913a0ff3-2c62-4d75-c34c-94a11daa5dad"
      },
      "id": "0j82ZbrpiGTQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([43, 51, 11,  1, 46, 39, 60, 47, 52, 45,  1, 40, 53, 58, 46,  1, 58, 46,\n",
            "        43,  1, 49, 43, 63,  0, 27, 44,  1, 53, 44, 44, 47, 41, 43, 56,  1, 39,\n",
            "        52, 42,  1, 53, 44, 44, 47, 41, 43,  6,  1, 57, 43, 58,  1, 39, 50, 50,\n",
            "         1, 46, 43, 39, 56, 58, 57,  1, 47,  5,  1, 58, 46, 43,  1, 57, 58, 39,\n",
            "        58, 43,  0, 32, 53,  1, 61, 46, 39, 58,  1, 58, 59, 52, 43,  1, 54, 50,\n",
            "        43, 39, 57, 43, 42,  1, 46, 47, 57,  1, 43, 39, 56, 11,  1, 58, 46, 39,\n",
            "        58,  1, 52, 53, 61,  1, 46, 43,  1, 61, 39, 57,  0, 32, 46, 43,  1, 47,\n",
            "        60, 63,  1, 61, 46, 47, 41, 46,  1, 46, 39, 42,  1, 46, 47, 42,  1, 51,\n",
            "        63,  1, 54, 56, 47, 52, 41, 43, 50, 63,  1, 58, 56, 59, 52, 49,  6,  0,\n",
            "        13, 52, 42,  1, 57, 59, 41, 49,  5, 42,  1, 51, 63,  1, 60, 43, 56, 42,\n",
            "        59, 56, 43,  1, 53, 59, 58,  1, 53, 52,  5, 58,  8,  1, 32, 46, 53, 59,\n",
            "         1, 39, 58, 58, 43, 52, 42,  5, 57, 58,  1, 52, 53, 58,  8,  0,  0, 25,\n",
            "        21, 30, 13, 26, 16, 13, 10,  0, 27,  6,  1, 45, 53, 53, 42,  1, 57, 47,\n",
            "        56,  6,  1, 21,  1, 42, 53,  8,  0,  0, 28, 30, 27, 31, 28, 17, 30, 27,\n",
            "        10,  0, 21,  1])\n",
            "tensor([51, 11,  1, 46, 39, 60, 47, 52, 45,  1, 40, 53, 58, 46,  1, 58, 46, 43,\n",
            "         1, 49, 43, 63,  0, 27, 44,  1, 53, 44, 44, 47, 41, 43, 56,  1, 39, 52,\n",
            "        42,  1, 53, 44, 44, 47, 41, 43,  6,  1, 57, 43, 58,  1, 39, 50, 50,  1,\n",
            "        46, 43, 39, 56, 58, 57,  1, 47,  5,  1, 58, 46, 43,  1, 57, 58, 39, 58,\n",
            "        43,  0, 32, 53,  1, 61, 46, 39, 58,  1, 58, 59, 52, 43,  1, 54, 50, 43,\n",
            "        39, 57, 43, 42,  1, 46, 47, 57,  1, 43, 39, 56, 11,  1, 58, 46, 39, 58,\n",
            "         1, 52, 53, 61,  1, 46, 43,  1, 61, 39, 57,  0, 32, 46, 43,  1, 47, 60,\n",
            "        63,  1, 61, 46, 47, 41, 46,  1, 46, 39, 42,  1, 46, 47, 42,  1, 51, 63,\n",
            "         1, 54, 56, 47, 52, 41, 43, 50, 63,  1, 58, 56, 59, 52, 49,  6,  0, 13,\n",
            "        52, 42,  1, 57, 59, 41, 49,  5, 42,  1, 51, 63,  1, 60, 43, 56, 42, 59,\n",
            "        56, 43,  1, 53, 59, 58,  1, 53, 52,  5, 58,  8,  1, 32, 46, 53, 59,  1,\n",
            "        39, 58, 58, 43, 52, 42,  5, 57, 58,  1, 52, 53, 58,  8,  0,  0, 25, 21,\n",
            "        30, 13, 26, 16, 13, 10,  0, 27,  6,  1, 45, 53, 53, 42,  1, 57, 47, 56,\n",
            "         6,  1, 21,  1, 42, 53,  8,  0,  0, 28, 30, 27, 31, 28, 17, 30, 27, 10,\n",
            "         0, 21,  1, 54])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68a93ca6-8a6d-4d6c-b7eb-b1aeae14ba71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68a93ca6-8a6d-4d6c-b7eb-b1aeae14ba71",
        "outputId": "b5cfb96f-eac0-4b91-fe7e-202037585132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.3272, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "rp-DRCUZ maQJr?F,E.HfmNDzd,HEvGNsA3Wj pE,CVEt\n",
            "nO&D-pq AQj!\n",
            ",Hj-fs?',ycT\n",
            "SRat:PdGdNSwPkmVGMyF;LOulLVg\n"
          ]
        }
      ],
      "source": [
        "# One head of self attention\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        # Query, key, and value are all linear layers.\n",
        "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
        "        # create a tril matrix of ones\n",
        "        # PyTorch naming convention because the tril is not a parameter\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B, T, head_size)\n",
        "        q = self.query(x) # (B, T, head_size)\n",
        "        v = self.value(x) # (B, T, head_size)\n",
        "\n",
        "        # Dot product the key and the query to get the weights\n",
        "        w = k @ q.transpose(-2, -1)  # (B,T,H) @ (B,H,T) = (B, T, T)\n",
        "\n",
        "        # Dividing by sqrt(head_size) for stability and making sure the variance stays close to zero\n",
        "        w = w * (C ** -0.5)\n",
        "\n",
        "        w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        w = F.softmax(w, dim = -1)\n",
        "        w = self.dropout(w)\n",
        "\n",
        "        out = w @ v # (B, T, T) @ (B, T, C) = (B, T, C) cuz B stays the same so essentially its a (T, T) @ (T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, no_of_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(no_of_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([head(x) for head in self.heads], dim = -1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, no_of_heads):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // no_of_heads\n",
        "        self.sa = MultiHeadAttention(no_of_heads, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, no_of_heads=no_of_heads) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets = None):\n",
        "        B,T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx) #(B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device = device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # idx and targets are of shape (B,T)\n",
        "            B,T,C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        #idx is (B,T)\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Cropping the idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :] # Becomes (B, C)\n",
        "            probs = F.softmax(logits, dim = -1)\n",
        "\n",
        "            # Sampling from distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
        "            idx = torch.cat((idx, idx_next), dim = 1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "logits, loss = m(xb, yb)\n",
        "print(loss)\n",
        "\n",
        "idx = torch.zeros((1,1), dtype = torch.long, device = device) # stands for the new line token \\n\n",
        "print(decode(m.generate(idx = idx, max_new_tokens = 100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    m.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = m(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    m.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "67hvygxew6g5"
      },
      "id": "67hvygxew6g5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "dFEETQv4w_rW"
      },
      "id": "dFEETQv4w_rW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de8b5531-4252-4153-b564-354b70919b52",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de8b5531-4252-4153-b564-354b70919b52",
        "outputId": "d7ade8f3-4f9a-44d6-af40-ebab7e9c5357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Train loss = 4.335792064666748, Val loss = 4.3321614265441895\n",
            "Step 500: Train loss = 1.6884046792984009, Val loss = 1.8422389030456543\n",
            "Step 1000: Train loss = 1.3595319986343384, Val loss = 1.5879207849502563\n",
            "Step 1500: Train loss = 1.238218903541565, Val loss = 1.5090975761413574\n",
            "Step 2000: Train loss = 1.1531693935394287, Val loss = 1.4877129793167114\n",
            "Step 2500: Train loss = 1.0890713930130005, Val loss = 1.4992848634719849\n",
            "Step 3000: Train loss = 1.0264713764190674, Val loss = 1.5077345371246338\n",
            "Step 3500: Train loss = 0.9688115119934082, Val loss = 1.5320357084274292\n",
            "Step 4000: Train loss = 0.9069042801856995, Val loss = 1.558619737625122\n",
            "Step 4500: Train loss = 0.8507474660873413, Val loss = 1.5785332918167114\n"
          ]
        }
      ],
      "source": [
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"Step {iter}: Train loss = {losses['train']}, Val loss = {losses['val']}\")\n",
        "\n",
        "    xb, yb = get_batch(\"train\")\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none = True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b6e8d7-46e3-4737-86b6-7ba3a0a3c287",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9b6e8d7-46e3-4737-86b6-7ba3a0a3c287",
        "outputId": "a2e170cf-a59d-496a-e7e7-fb7147c00c39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "RIVERSS:\n",
            "I cannot nor tetding those sit you shalt do\n",
            "To well o'er the top offenders, for my honour.\n",
            "We have no fger than groan the steeds and well, and\n",
            "chelt his head one. Let me be spoke for another;\n",
            "he was another being formed, but banish him;\n",
            "he, for a pippe. I see, thought leave thee saw\n",
            "In those that I condemn to unwish! If thou be\n",
            "no great opparent to be in sudden formity,\n",
            "who lately wear theirs past that revenge this outwo\n",
            "compass.\n",
            "\n",
            "ESCALUS:\n",
            "Why do you this?\n",
            "\n",
            "LUCIO:\n",
            "Doth ship forfeitly that consul, Signio?\n",
            "\n",
            "ANGELO:\n",
            "Well; bring of all.\n",
            "\n",
            "ESCALUS:\n",
            "It is for your wisdom be the nopent. The wish, the\n",
            "apperonce is a guilty will tread yourself and\n",
            "profit, your sea, that the wretchedness in our sleep,\n",
            "the fier eventon you as the sword or waking\n",
            "and her constempore by what short is no year.\n",
            "\n",
            "First Senator:\n",
            "Weeping we well ender, were the case you--\n",
            "He as sure.\n",
            "\n",
            "AUTOLYCUS:\n",
            "Gentle Pardon, my friend Potist ballads;--here is in\n",
            "performity; how art like to visit the worst along,\n",
            "that Aufidius, i'll seek to the people hour of the world,\n",
            "because herd in French with a gage. Sile, sometime!\n",
            "\n",
            "First Servingman:\n",
            "Happily away a would to make you woman?\n",
            "\n",
            "Second Servingman:\n",
            "Now are you gerant friends; I hope, sTeeld.\n",
            "\n",
            "Second Servingman:\n",
            "In the sky cententing'd for his country, I\n",
            "have almost forgotten his resolutoming friends, thature\n",
            "passishes; you are puff'd for the chancemony; whom the\n",
            "kings becond rootm of rast was an employman. Ladders admine\n",
            "to put what you to my wench, misgive me up\n",
            "not with me: but only the abprisonment die\n",
            "in him. Follows true, Post lay along:\n",
            "Look pale hour, shall we hear the bog\n",
            "of his pon the sen flower.\n",
            "\n",
            "POLIXENES:\n",
            "O, good my lords,\n",
            "Thou wilt read to the banish,\n",
            "He tender with his that flattered me by hell\n",
            "Some practise fit for him; and formally proceed\n",
            "Such factions as we will shrink.\n",
            "\n",
            "POLIXENES:\n",
            "What is your will\n",
            "Came with Aufidius like me asha, as his merripely\n",
            "embashed.\n",
            "\n",
            "COMINIUS:\n",
            "Pray you, do.\n",
            "\n",
            "CORIOLANUS:\n",
            "Marcius Aufidius,\n",
            "'Tis straight for better.\n",
            "\n",
            "COMINIUS:\n",
            "'Tis he singled:\n",
            "Submisibility, the king I had scareth not race\n",
            "offended to the state, and to mustock my body\n",
            "Then, all the blastary-will try with thee.\n",
            "\n",
            "Citizens:\n",
            "This, mine wind two sudden drops one another gentleman;\n",
            "Which, so would I exhapely me, I wear no moe\n",
            "Of their perpipositions on my mistress' heart\n",
            "That modest them consul. The gship is of roaring\n",
            "The rude mercy did recreant, which kill'd\n",
            "To think the breath: aast one thing still trembles,\n",
            "He waste upon her corn, who cause her wrong'd\n",
            "From him, as now a herd would offend to order?\n",
            "\n",
            "ISABELLA:\n",
            "Would he knew ere blest as which is in harved,\n",
            "Or she cannot choose it. Be it lost for this familiar,\n",
            "Even for the queen'? I shall not prove you to't.\n",
            "Never bear it please your servant daughter to meet,\n",
            "you must know letters in a soldier. Hark!\n",
            "\n",
            "LADY CAPULET:\n",
            "Hortensio for Latin, she loves your passage;\n",
            "And I hear, love to your pronest, Barnardine,\n",
            "howsoever your pleasure into are to known,\n",
            "But ere so answer I.\n",
            "\n",
            "LEONTES:\n",
            "So.\n",
            "\n",
            "LEONTES:\n",
            "One white ground about'd more:\n",
            "Why was for my pains, hath none of high esteed\n",
            "By those that way he is the tribunes? God--\n",
            "\n",
            "TRANIO:\n",
            "Does it not bitter to speak a Jack.\n",
            "O God his hope, that to-morrow! whose that friends\n",
            "Call goes to be asleep a fearful of good\n",
            "More innovorcement of age battle put for his legs\n",
            "and but heir, though you hast married of mine to-morrow, and\n",
            "to be silent; by you are in angry of the maid:\n",
            "prepare not to keep him to know the great a time.\n",
            "First which if to me cry bows to sing from he hours as thences\n",
            "since that the loud recrapting to his health\n",
            "will lost his master; but I do confess, his mast\n",
            "hear me speak till man stand the world.\n",
            "\n",
            "SURET:\n",
            "Say, you have your own course.\n",
            "\n",
            "Both Citizen:\n",
            "I too safegue your will: is in Parpie, I pray the\n",
            "customary I carch in Padua his honour at your affection:\n",
            "Mine inheritable the eagle abhars gaunt I have\n",
            "your pardone for't.\n",
            "\n",
            "Clown:\n",
            "You have done about 't.\n",
            "\n",
            "AUTOLYCUS:\n",
            "Nay, i' fellow, here's one friar, if thou know\n",
            "married to be her fault, but it should not trood to,\n",
            "unless than hopes his fortune with a dangerous storm\n",
            "yours, being through the world of, lest abreech,\n",
            "says his business. He hath songs the contrary?\n",
            "\n",
            "First Murderer:\n",
            "In this reward of this dagger of that doubtful:\n",
            "bring counsell.\n",
            "\n",
            "Second Murderer:\n",
            "You shall not see, for yond in his grave speech\n",
            "stripper; you mistrust with me.\n",
            "\n",
            "First Murderer:\n",
            "You know you much worthy tongue.\n",
            "\n",
            "LADY CAPULET:\n",
            "No, in mind there but that ksays strife, nor walk.\n",
            "\n",
            "Second Murderer:\n",
            "Shall you do it go?\n",
            "\n",
            "Second Murderer:\n",
            "Here come two knows.\n",
            "\n",
            "CLARENCE:\n",
            "First, away! a thrift; where a most liking to here.\n",
            "\n",
            "FLORIZEL:\n",
            "She is here.\n",
            "\n",
            "PETER:\n",
            "Prithee, be gone:\n",
            "Let's see: he has, Claudio, and coming question,\n",
            "Come I to voice hither.\n",
            "\n",
            "KING RICHASTINGELE:\n",
            "God keep my aed, lest the welcome.\n",
            "\n",
            "Pursuivant:\n",
            "I am behoposing to your hands with you:\n",
            "With alongs but star, whence gentlewoman's enemy.\n",
            "\n",
            "JULIET:\n",
            "As past lease, aside, honourable array,\n",
            "In all banks to hide you of his charge;\n",
            "For more empty a back with light from your headstill\n",
            "Let us away the corpiTable unipped battle;\n",
            "The streak title shallow more, on my appointed loss\n",
            "Than wronging our confess care of Kent, I'll put them\n",
            "Disposition to this harmony:\n",
            "Please this place he hath tempose, leased us to wail\n",
            "On his villain; having from the heart to go\n",
            "And unstill 't speak,\n",
            "And in the posterity is no repless.\n",
            "\n",
            "CATESBY:\n",
            "When you know, my lord,\n",
            "Grant of thy banden correctings, which, but with a\n",
            "maiuser whence this breeding, it were dark,\n",
            "grave unbrooch'd, harsh it, and, a making\n",
            "Solemner, tediing it, to concein at I have good\n",
            "think: you're promise in the war's, and find\n",
            "I consent to bower you.\n",
            "\n",
            "SICINIUS:\n",
            "Take them in a kind understand servily:\n",
            "In what am English, we must have constracted,\n",
            "And let them be more ado.\n",
            "\n",
            "LEONTES:\n",
            "O, you grave and all traitors\n",
            "Than fraudy to the wife assist misle,\n",
            "That last shine blazis with his discourse,\n",
            "Lest the truth in the event, whose honour\n",
            "We tread to must by the dream of worthy troop\n",
            "Unto the good Duke of York's thus wedged concerns.\n",
            "\n",
            "WARWICK:\n",
            "If 'twere to heaven to go deal together,\n",
            "On The villains their senators call the Duke of York's face,\n",
            "Have not taste to our foes. Bianca, let thought her fond\n",
            "A power to be too reprehent an out bowless.\n",
            "\n",
            "KING RICHARD III:\n",
            "You did kill it joy, he shall be able to be king.\n",
            "\n",
            "THRIV:\n",
            "Away! long live these war scarf?\n",
            "\n",
            "GLOUCESTER:\n",
            "But the times he hath control'd it that usurpets it\n",
            "In that they shall substant us all will;\n",
            "Once proclaim us to the warlike head,\n",
            "With his summer arm chided likeward exples and\n",
            "Tears time from the bastardy of orne and fray.\n",
            "\n",
            "KING HENRY VI:\n",
            "Happaring for the court?\n",
            "\n",
            "MONTAGUE:\n",
            "Montague more are venuto, and made a loss.\n",
            "Tybalt, the one that you have married the tuff\n",
            "To-morrow to go with that would have sweet!\n",
            "Might have the most follow to my sight.\n",
            "\n",
            "KING HENRY VI:\n",
            "\n",
            "QUEEN MARGILIZABETH:\n",
            "Who shall be done?\n",
            "\n",
            "BUCKINGHAM:\n",
            "Happy\n",
            "That Romeo's word of cries!\n",
            "\n",
            "KING RICHARD III:\n",
            "Well, that all hare here had I live.\n",
            "\n",
            "QUEEN:\n",
            "But first in that every man have been thou pluck'dst fallow,\n",
            "Being never fear'd, like to executes here her cleans?\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "How to whom child I blind our noble king!\n",
            "\n",
            "KING RICHARD II:\n",
            "We are as ruled in tears.\n",
            "If thou to fight a mock to this eyes all play\n",
            "To meet as Edward will, and to set the brain;\n",
            "Whose, by the world, I vide thy beard a mother,\n",
            "Whose friend, unless thanks. Though Paritia, I\n",
            "Art a grust all things as guiltles; and yet thou hast\n",
            "for me, makest thou hitherry our Montague.\n",
            "\n",
            "BOLIONUS:\n",
            "Well, she are first:\n",
            "And he that, spread for for a thing can worsea,\n",
            "I censure him, and gred him yet his woes way.\n",
            "\n",
            "Second Citizen:\n",
            "He has he dead constanted with Paday prince; with groans\n",
            "He was like tonder upon Hectorshings, as the world\n",
            "Hath th kill upon his glass throne true-treets of his\n",
            "kinsman, he hath beguile.\n",
            "\n",
            "First Senator:\n",
            "He shall be no more condition; but when his wife\n",
            "Had threw you allow'd, daughted Jesu,\n",
            "That vantage have as prophetesty; not for these\n",
            "Which God's angry of senses courtesy.\n",
            "\n",
            "CAPULET:\n",
            "Peace, pardon, good fellow.\n",
            "\n",
            "FRIAR PETER:\n",
            "Not the Volscian till our provost?\n",
            "\n",
            "PRINCE:\n",
            "Hark, my lord; here is he like to prove.\n",
            "Nurse, more, sir: welcome, and better for the\n",
            "mother of the world. Pray you, sir, fare your fevt was good\n",
            "on the power you, lest you may not weep in gracious back.\n",
            "\n",
            "QUEEN:\n",
            "Cleadings, speak amazek nonecture. What's horse! He'ld to 'The torny\n",
            "Of Earlish blood, to go with him: he hath side his head,\n",
            "and trouble comfort his; past I his prating head\n",
            "And talk'd against him. Be heard o'er\n",
            "The shepherd's masters. Has he diserved lives\n",
            "A little place to the sea he-broll'n falses,\n",
            "Eat our Rome and in Nappeter, pleased uinto\n",
            "Where we are, his brother Montagues. The north,\n",
            "Withdraw we come: though in hand wrong, daughter, great,\n",
            "Intender them, for a worthy time day:\n",
            "To God's wife, to wonder or what title reptive\n",
            "That Phaethon bash and Exeter;\n",
            "How shall we hedtle trumph his wife abofull ksword;\n",
            "And, truly deposing how I play his foe.\n",
            "\n",
            "BUSHY:\n",
            "Master master, progued duke is enech well;\n",
            "I cannot believe me, and though I had rack it:\n",
            "For you disdain truth tread out me shows upon my very\n",
            "death to see Juliet, as I am.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "O, well unpemptul! the region am I seen,\n",
            "I will perform't with the wretch of his good.\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "Cudit flympure came to you and gnatly send\n",
            "Both cadness to good all your England,\n",
            "And prently of the Duke of Gloucester!\n",
            "\n",
            "BENVOLIO:\n",
            "O, get the French his inforce will accompare\n",
            "To serpease his courts that true-pluck'd spent bosom,\n",
            "And balt to practive. I will, good mother;\n",
            "No remedy good, but not anothing for I could\n",
            "Bury to some queen and good old way.\n",
            "\n",
            "CLEOMENES:\n",
            "Here comes my lord, Sir Warwick a traitor Here,\n",
            "Broth Warwick here for our substitute sums\n",
            "And secret no learn of Hereford his gorten corse:\n",
            "And thou art affectional villain,\n",
            "Wert thou talk'st of a man; why, not for his name?\n",
            "\n",
            "Second Murderer:\n",
            "No man is safety, and so such strange,\n",
            "Stabbing to the weaked like state and whi\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = idx, max_new_tokens = 10000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "O2_u-uEsOGuq"
      },
      "id": "O2_u-uEsOGuq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e67477f-9645-4811-955b-5fc661dc1426",
      "metadata": {
        "id": "2e67477f-9645-4811-955b-5fc661dc1426"
      },
      "outputs": [],
      "source": [
        "B,T,C = 4,8,32\n",
        "\n",
        "print(w.shape)\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Our model: \\n\\n\", model, '\\n')\n",
        "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX7k6UJrEQsq",
        "outputId": "fde2585a-ea75-41dc-fc89-f12f5ccf13c1"
      },
      "id": "oX7k6UJrEQsq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our model: \n",
            "\n",
            " BigramLanguageModel(\n",
            "  (token_embedding_table): Embedding(65, 384)\n",
            "  (position_embedding_table): Embedding(256, 384)\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-5): 6 x Head(\n",
            "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-5): 6 x Head(\n",
            "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-5): 6 x Head(\n",
            "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-5): 6 x Head(\n",
            "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (4): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-5): 6 x Head(\n",
            "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (5): Block(\n",
            "      (sa): MultiHeadAttention(\n",
            "        (heads): ModuleList(\n",
            "          (0-5): 6 x Head(\n",
            "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
            "            (dropout): Dropout(p=0.2, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (dropout): Dropout(p=0.2, inplace=False)\n",
            "      )\n",
            "      (ffwd): FeedForward(\n",
            "        (net): Sequential(\n",
            "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (3): Dropout(p=0.2, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
            ") \n",
            "\n",
            "The state dict keys: \n",
            "\n",
            " odict_keys(['token_embedding_table.weight', 'position_embedding_table.weight', 'blocks.0.sa.heads.0.tril', 'blocks.0.sa.heads.0.key.weight', 'blocks.0.sa.heads.0.query.weight', 'blocks.0.sa.heads.0.value.weight', 'blocks.0.sa.heads.1.tril', 'blocks.0.sa.heads.1.key.weight', 'blocks.0.sa.heads.1.query.weight', 'blocks.0.sa.heads.1.value.weight', 'blocks.0.sa.heads.2.tril', 'blocks.0.sa.heads.2.key.weight', 'blocks.0.sa.heads.2.query.weight', 'blocks.0.sa.heads.2.value.weight', 'blocks.0.sa.heads.3.tril', 'blocks.0.sa.heads.3.key.weight', 'blocks.0.sa.heads.3.query.weight', 'blocks.0.sa.heads.3.value.weight', 'blocks.0.sa.heads.4.tril', 'blocks.0.sa.heads.4.key.weight', 'blocks.0.sa.heads.4.query.weight', 'blocks.0.sa.heads.4.value.weight', 'blocks.0.sa.heads.5.tril', 'blocks.0.sa.heads.5.key.weight', 'blocks.0.sa.heads.5.query.weight', 'blocks.0.sa.heads.5.value.weight', 'blocks.0.sa.proj.weight', 'blocks.0.sa.proj.bias', 'blocks.0.ffwd.net.0.weight', 'blocks.0.ffwd.net.0.bias', 'blocks.0.ffwd.net.2.weight', 'blocks.0.ffwd.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.sa.heads.0.tril', 'blocks.1.sa.heads.0.key.weight', 'blocks.1.sa.heads.0.query.weight', 'blocks.1.sa.heads.0.value.weight', 'blocks.1.sa.heads.1.tril', 'blocks.1.sa.heads.1.key.weight', 'blocks.1.sa.heads.1.query.weight', 'blocks.1.sa.heads.1.value.weight', 'blocks.1.sa.heads.2.tril', 'blocks.1.sa.heads.2.key.weight', 'blocks.1.sa.heads.2.query.weight', 'blocks.1.sa.heads.2.value.weight', 'blocks.1.sa.heads.3.tril', 'blocks.1.sa.heads.3.key.weight', 'blocks.1.sa.heads.3.query.weight', 'blocks.1.sa.heads.3.value.weight', 'blocks.1.sa.heads.4.tril', 'blocks.1.sa.heads.4.key.weight', 'blocks.1.sa.heads.4.query.weight', 'blocks.1.sa.heads.4.value.weight', 'blocks.1.sa.heads.5.tril', 'blocks.1.sa.heads.5.key.weight', 'blocks.1.sa.heads.5.query.weight', 'blocks.1.sa.heads.5.value.weight', 'blocks.1.sa.proj.weight', 'blocks.1.sa.proj.bias', 'blocks.1.ffwd.net.0.weight', 'blocks.1.ffwd.net.0.bias', 'blocks.1.ffwd.net.2.weight', 'blocks.1.ffwd.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.sa.heads.0.tril', 'blocks.2.sa.heads.0.key.weight', 'blocks.2.sa.heads.0.query.weight', 'blocks.2.sa.heads.0.value.weight', 'blocks.2.sa.heads.1.tril', 'blocks.2.sa.heads.1.key.weight', 'blocks.2.sa.heads.1.query.weight', 'blocks.2.sa.heads.1.value.weight', 'blocks.2.sa.heads.2.tril', 'blocks.2.sa.heads.2.key.weight', 'blocks.2.sa.heads.2.query.weight', 'blocks.2.sa.heads.2.value.weight', 'blocks.2.sa.heads.3.tril', 'blocks.2.sa.heads.3.key.weight', 'blocks.2.sa.heads.3.query.weight', 'blocks.2.sa.heads.3.value.weight', 'blocks.2.sa.heads.4.tril', 'blocks.2.sa.heads.4.key.weight', 'blocks.2.sa.heads.4.query.weight', 'blocks.2.sa.heads.4.value.weight', 'blocks.2.sa.heads.5.tril', 'blocks.2.sa.heads.5.key.weight', 'blocks.2.sa.heads.5.query.weight', 'blocks.2.sa.heads.5.value.weight', 'blocks.2.sa.proj.weight', 'blocks.2.sa.proj.bias', 'blocks.2.ffwd.net.0.weight', 'blocks.2.ffwd.net.0.bias', 'blocks.2.ffwd.net.2.weight', 'blocks.2.ffwd.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.sa.heads.0.tril', 'blocks.3.sa.heads.0.key.weight', 'blocks.3.sa.heads.0.query.weight', 'blocks.3.sa.heads.0.value.weight', 'blocks.3.sa.heads.1.tril', 'blocks.3.sa.heads.1.key.weight', 'blocks.3.sa.heads.1.query.weight', 'blocks.3.sa.heads.1.value.weight', 'blocks.3.sa.heads.2.tril', 'blocks.3.sa.heads.2.key.weight', 'blocks.3.sa.heads.2.query.weight', 'blocks.3.sa.heads.2.value.weight', 'blocks.3.sa.heads.3.tril', 'blocks.3.sa.heads.3.key.weight', 'blocks.3.sa.heads.3.query.weight', 'blocks.3.sa.heads.3.value.weight', 'blocks.3.sa.heads.4.tril', 'blocks.3.sa.heads.4.key.weight', 'blocks.3.sa.heads.4.query.weight', 'blocks.3.sa.heads.4.value.weight', 'blocks.3.sa.heads.5.tril', 'blocks.3.sa.heads.5.key.weight', 'blocks.3.sa.heads.5.query.weight', 'blocks.3.sa.heads.5.value.weight', 'blocks.3.sa.proj.weight', 'blocks.3.sa.proj.bias', 'blocks.3.ffwd.net.0.weight', 'blocks.3.ffwd.net.0.bias', 'blocks.3.ffwd.net.2.weight', 'blocks.3.ffwd.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.sa.heads.0.tril', 'blocks.4.sa.heads.0.key.weight', 'blocks.4.sa.heads.0.query.weight', 'blocks.4.sa.heads.0.value.weight', 'blocks.4.sa.heads.1.tril', 'blocks.4.sa.heads.1.key.weight', 'blocks.4.sa.heads.1.query.weight', 'blocks.4.sa.heads.1.value.weight', 'blocks.4.sa.heads.2.tril', 'blocks.4.sa.heads.2.key.weight', 'blocks.4.sa.heads.2.query.weight', 'blocks.4.sa.heads.2.value.weight', 'blocks.4.sa.heads.3.tril', 'blocks.4.sa.heads.3.key.weight', 'blocks.4.sa.heads.3.query.weight', 'blocks.4.sa.heads.3.value.weight', 'blocks.4.sa.heads.4.tril', 'blocks.4.sa.heads.4.key.weight', 'blocks.4.sa.heads.4.query.weight', 'blocks.4.sa.heads.4.value.weight', 'blocks.4.sa.heads.5.tril', 'blocks.4.sa.heads.5.key.weight', 'blocks.4.sa.heads.5.query.weight', 'blocks.4.sa.heads.5.value.weight', 'blocks.4.sa.proj.weight', 'blocks.4.sa.proj.bias', 'blocks.4.ffwd.net.0.weight', 'blocks.4.ffwd.net.0.bias', 'blocks.4.ffwd.net.2.weight', 'blocks.4.ffwd.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.sa.heads.0.tril', 'blocks.5.sa.heads.0.key.weight', 'blocks.5.sa.heads.0.query.weight', 'blocks.5.sa.heads.0.value.weight', 'blocks.5.sa.heads.1.tril', 'blocks.5.sa.heads.1.key.weight', 'blocks.5.sa.heads.1.query.weight', 'blocks.5.sa.heads.1.value.weight', 'blocks.5.sa.heads.2.tril', 'blocks.5.sa.heads.2.key.weight', 'blocks.5.sa.heads.2.query.weight', 'blocks.5.sa.heads.2.value.weight', 'blocks.5.sa.heads.3.tril', 'blocks.5.sa.heads.3.key.weight', 'blocks.5.sa.heads.3.query.weight', 'blocks.5.sa.heads.3.value.weight', 'blocks.5.sa.heads.4.tril', 'blocks.5.sa.heads.4.key.weight', 'blocks.5.sa.heads.4.query.weight', 'blocks.5.sa.heads.4.value.weight', 'blocks.5.sa.heads.5.tril', 'blocks.5.sa.heads.5.key.weight', 'blocks.5.sa.heads.5.query.weight', 'blocks.5.sa.heads.5.value.weight', 'blocks.5.sa.proj.weight', 'blocks.5.sa.proj.bias', 'blocks.5.ffwd.net.0.weight', 'blocks.5.ffwd.net.0.bias', 'blocks.5.ffwd.net.2.weight', 'blocks.5.ffwd.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'ln_f.weight', 'ln_f.bias', 'lm_head.weight', 'lm_head.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "torch.save(m.state_dict(), 'trainedGenerator(1.57).pth')\n",
        "# download checkpoint file\n",
        "files.download('trainedGenerator(1.57).pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0CsrvM9GWnBf",
        "outputId": "2f8054be-6a58-47e2-c97f-c0ab5d427693"
      },
      "id": "0CsrvM9GWnBf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8fa70039-84fa-4c5d-ae80-89e25fee46d1\", \"trainedGenerator(1.57).pth\", 52675224)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of parameters in the model\n",
        "sum(p.numel() for p in m.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA9COj5yYsYz",
        "outputId": "2a473fbf-55a7-40a5-c801-8b78ffbd61c3"
      },
      "id": "SA9COj5yYsYz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10788929"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}